{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mx5Gn7TiyIGv"
      },
      "source": [
        "# Подготовка датасета"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXMalkUJ1tFB"
      },
      "source": [
        "## Установка transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TMYYJ7_do08a"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers datasets\n",
        "!pip install wget"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxayjYMSEw9r"
      },
      "source": [
        "## Загрузка оригинального датасета с  сайта сбера\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W70LYEcMEwzk"
      },
      "outputs": [],
      "source": [
        "import wget\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "def bar_progress(current, total, width=80):\n",
        "    progress_message = \"Downloading: %d%% [%d / %d] bytes\" % (current / total * 100, current, total)\n",
        "    sys.stdout.write(\"\\r\" + progress_message)\n",
        "    sys.stdout.flush()\n",
        "\n",
        "print('Beginning download...')\n",
        "\n",
        "url = 'https://dsworks.s3pd01.sbercloud.ru/aij2021/AITrain_train/AITrain_train.zip'\n",
        "wget.download(url, bar=bar_progress)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bj3YOdnyjp-"
      },
      "source": [
        "## Загрузка оригинального датасета с google диска\n",
        "\n",
        "Последующая его обработка\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dXrdkrFIXqsE"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xV47u-3lMfde"
      },
      "outputs": [],
      "source": [
        "!unzip -q /content/drive/MyDrive/AITrain_train.zip -d /content/dataset/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tKx0fQzZF_Mf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQyK7qp0MfT0"
      },
      "outputs": [],
      "source": [
        "images_dir = \"/content/dataset/train_data/masks/\"\n",
        "\n",
        "IMAGES_NAME = os.listdir(images_dir)\n",
        "\n",
        "i = 0\n",
        "l = len(IMAGES_NAME)\n",
        "\n",
        "\n",
        "for img in IMAGES_NAME:\n",
        "    if i % 100 == 0:\n",
        "        print(f\"{i} / {l}: {i / l * 100}%\")\n",
        "    im = Image.open(images_dir + img)\n",
        "    im, _, _ = im.split()\n",
        "    im.save(images_dir + img)\n",
        "\n",
        "    i += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zYtssQ39LCUx"
      },
      "outputs": [],
      "source": [
        "images_dir = \"/content/dataset/train_data/masks/\"\n",
        "\n",
        "IMAGES_NAME = os.listdir(images_dir)\n",
        "\n",
        "\n",
        "i = 0\n",
        "for img in IMAGES_NAME:\n",
        "    if i % 100 == 0:\n",
        "        print(f\"{i} / {l}: {i / l * 100}%\")\n",
        "    im = Image.open(images_dir + img)\n",
        "\n",
        "    data = np.array(im)\n",
        "\n",
        "    train = 10 # Original value of train\n",
        "    rail_add = 6 # Original value of additional railroad\n",
        "    rail = 7 # Original value of railroad\n",
        "    black = 0 # Value that we want to replace it with\n",
        "    gray = data[:,:]\n",
        "    mask = (gray == train)\n",
        "    data[:,:][mask] = [black]\n",
        "\n",
        "    gray = data[:,:]\n",
        "    mask = (gray == rail_add)\n",
        "    data[:,:][mask] = [rail]\n",
        "    im = Image.fromarray(data)\n",
        "    \n",
        "    im.save(images_dir + img)\n",
        "    i += 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FSh_OD_o040I"
      },
      "outputs": [],
      "source": [
        "!zip -q -T -m -r /content/segmentation.zip /content/dataset/\n",
        "!cp /content/segmentation.zip /content/drive/MyDrive\n",
        "drive.flush_and_unmount()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqLjSZ7lcpex"
      },
      "source": [
        "## Загрузка обработанного датасета"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bs8C9LDbzFT1"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W4QfGb6fzGNi"
      },
      "outputs": [],
      "source": [
        "!unzip -q /content/drive/MyDrive/segmentation.zip -d /"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VyEXNfwpWsl"
      },
      "source": [
        "## Создание PyTorch dataset and dataloaders\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QjHHo_eLpYMa"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "class SemanticSegmentationDataset(Dataset):\n",
        "    \"\"\"Image (semantic) segmentation dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, root_dir, feature_extractor, train=True, test=False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            root_dir (string): Root directory of the dataset containing the images + annotations.\n",
        "            feature_extractor (SegFormerFeatureExtractor): feature extractor to prepare images + segmentation maps.\n",
        "            train (bool): Whether to load \"training\" or \"validation\" images + annotations.\n",
        "        \"\"\"\n",
        "        self.root_dir = root_dir\n",
        "        self.feature_extractor = feature_extractor\n",
        "        self.train = train\n",
        "\n",
        "        self.img_dir = os.path.join(self.root_dir, \"images\")\n",
        "        self.ann_dir = os.path.join(self.root_dir, \"masks\")\n",
        "        \n",
        "        # read images\n",
        "        image_file_names = []\n",
        "        for root, dirs, files in os.walk(self.img_dir):\n",
        "          image_file_names.extend(files)\n",
        "        if train:\n",
        "          self.images = sorted(image_file_names)[:int(len(image_file_names) * 0.7)]\n",
        "        elif not test:\n",
        "          self.images = sorted(image_file_names)[int(len(image_file_names) * 0.7):int(len(image_file_names) * 0.9)]\n",
        "        else:\n",
        "          self.images = sorted(image_file_names)[int(len(image_file_names) * 0.9):]\n",
        "        \n",
        "        # read annotations\n",
        "        annotation_file_names = []\n",
        "        for root, dirs, files in os.walk(self.ann_dir):\n",
        "          annotation_file_names.extend(files)\n",
        "        if train:\n",
        "          self.annotations = sorted(annotation_file_names)[:int(len(image_file_names) * 0.7)]\n",
        "        elif not test:\n",
        "          self.annotations = sorted(annotation_file_names)[int(len(annotation_file_names) * 0.7):int(len(annotation_file_names) * 0.9)]\n",
        "        else:\n",
        "          self.annotations = sorted(annotation_file_names)[int(len(annotation_file_names) * 0.9):]\n",
        "\n",
        "        assert len(self.images) == len(self.annotations), \"There must be as many images as there are segmentation maps\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \n",
        "        image = Image.open(os.path.join(self.img_dir, self.images[idx]))\n",
        "        segmentation_map = Image.open(os.path.join(self.ann_dir, self.annotations[idx]))\n",
        "\n",
        "        # randomly crop + pad both image and segmentation map to same size\n",
        "        encoded_inputs = self.feature_extractor(image, segmentation_map, return_tensors=\"pt\")\n",
        "\n",
        "        for k,v in encoded_inputs.items():\n",
        "          encoded_inputs[k].squeeze_() # remove batch dimension\n",
        "\n",
        "        return encoded_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8GpVF2Dpkvs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105,
          "referenced_widgets": [
            "2970c7949a1947579074284d81928576",
            "029a14c9b4ca402fac92380939937790",
            "92a8cb50c657418f97269ee8002d19fc",
            "cd7be3d3e6294383811fe0a99b3b2607",
            "5e632cafa98a4956aa2076b45e208b94",
            "71a65d53ef994b369f5a93f37249156d",
            "a4ae7e43d9024df4bb89c77926fbccf1",
            "3a273047763049af9ed9a41fba608dda",
            "6702fc57f1734c2e84863a717ae20703",
            "c1102ac4653b4f8d9f3092d18b30d932",
            "707c1970f1e84475bcb7778a71611ce6"
          ]
        },
        "outputId": "581e7d1c-3284-4820-b5a4-ba440e38c911"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/272 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2970c7949a1947579074284d81928576"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/models/segformer/image_processing_segformer.py:102: FutureWarning: The `reduce_labels` parameter is deprecated and will be removed in a future version. Please use `do_reduce_labels` instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import SegformerFeatureExtractor\n",
        "model_type = \"b2\"\n",
        "root_dir = '/content/dataset/train_data'\n",
        "\n",
        "feature_extractor = SegformerFeatureExtractor.from_pretrained(f\"nvidia/mit-{model_type}\")\n",
        "feature_extractor.reduce_labels = True\n",
        "feature_extractor.size = 512\n",
        "\n",
        "train_dataset = SemanticSegmentationDataset(root_dir=root_dir, feature_extractor=feature_extractor)\n",
        "valid_dataset = SemanticSegmentationDataset(root_dir=root_dir, feature_extractor=feature_extractor, train=False)\n",
        "test_dataset = SemanticSegmentationDataset(root_dir=root_dir, feature_extractor=feature_extractor, train=False, test=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7NqpQETFcoS"
      },
      "outputs": [],
      "source": [
        "print(\"Number of training examples:\", len(train_dataset))\n",
        "print(\"Number of validation examples:\", len(valid_dataset))\n",
        "print(\"Number of test examples:\", len(test_dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hfr3l91nrS-C"
      },
      "source": [
        "Next, we define corresponding dataloaders."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q4cU8hU3rZF-"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size=2)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HpjB47KB6iy3"
      },
      "outputs": [],
      "source": [
        "batch = next(iter(train_dataloader))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7R9lxUSq4mgr"
      },
      "source": [
        "## Define the model\n",
        "\n",
        "Here we load the model, and equip the encoder with weights pre-trained on ImageNet-1k (we take the smallest variant, `nvidia/mit-b0` here, but you can take a bigger one like `nvidia/mit-b5` from the [hub](https://huggingface.co/models?other=segformer)). We also set the `id2label` and `label2id` mappings, which will be useful when performing inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81dWx1WBqIos"
      },
      "outputs": [],
      "source": [
        "from transformers import SegformerForSemanticSegmentation\n",
        "import json\n",
        "from huggingface_hub import cached_download, hf_hub_url\n",
        "\n",
        "filename = \"sber-id2label.json\"\n",
        "id2label = json.load(open(filename, \"r\"))\n",
        "id2label = {int(k): v for k, v in id2label.items()}\n",
        "label2id = {v: k for k, v in id2label.items()}\n",
        "\n",
        "# define model\n",
        "model = SegformerForSemanticSegmentation.from_pretrained(f\"nvidia/mit-{model_type}\",\n",
        "                                                         num_labels=16, \n",
        "                                                         id2label=id2label, \n",
        "                                                         label2id=label2id,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONgyTbnRxXvF"
      },
      "source": [
        "## Fine-tune the model\n",
        "\n",
        "Here we fine-tune the model in native PyTorch, using the AdamW optimizer. We use the same learning rate as the one reported in the [paper](https://arxiv.org/abs/2105.15203).\n",
        "\n",
        "It's also very useful to track metrics during training. For semantic segmentation, typical metrics include the mean intersection-over-union (mIoU) and pixel-wise accuracy. These are available in the Datasets library. We can load it as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iOdgd24YOSsw"
      },
      "outputs": [],
      "source": [
        "from datasets import load_metric\n",
        "\n",
        "metric = load_metric(\"mean_iou\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4fDdkjWYetSF"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import torch\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gHdp6-w0wDei"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tqdm.notebook import tqdm\n",
        "# define optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.00006)\n",
        "load = True\n",
        "if load:\n",
        "    checkpoint = torch.load(f\"/content/drive/MyDrive/rails/mit-{model_type}/last_checkpoint.pt\")\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    start_idx = checkpoint['idx']\n",
        "    if start_idx == 0:\n",
        "      start_epoch = checkpoint['epoch'] + 1\n",
        "    else:\n",
        "      start_epoch = checkpoint['epoch']\n",
        "    loss = checkpoint['loss']\n",
        "else:\n",
        "    start_epoch = 0\n",
        "    start_idx = 0\n",
        "\n",
        "# move model to GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "model.train()\n",
        "for epoch in range(start_epoch, 10):  # loop over the dataset multiple times\n",
        "    print(\"Epoch:\", epoch)\n",
        "    for idx, batch in enumerate(tqdm(train_dataloader)):\n",
        "        if start_idx != 0 and idx <= start_idx:\n",
        "          continue\n",
        "        else:\n",
        "          start_idx = 0\n",
        "        # get the inputs;\n",
        "        pixel_values = batch[\"pixel_values\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = model(pixel_values=pixel_values, labels=labels)\n",
        "        loss, logits = outputs.loss, outputs.logits\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # evaluate\n",
        "        with torch.no_grad():\n",
        "          upsampled_logits = nn.functional.interpolate(logits, size=labels.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
        "          predicted = upsampled_logits.argmax(dim=1)\n",
        "          \n",
        "          metric.add_batch(predictions=predicted.detach().cpu().numpy(), references=labels.detach().cpu().numpy())\n",
        "        if idx % 400 == 0:\n",
        "          metrics = metric.compute(num_labels=len(id2label), \n",
        "                                   ignore_index=255,\n",
        "                                   reduce_labels=False)\n",
        "          \n",
        "          print(idx, \"\")\n",
        "          print(\"Loss:\", loss.item())\n",
        "          print(\"Mean_iou:\", metrics[\"mean_iou\"])\n",
        "          print(\"Mean accuracy:\", metrics[\"mean_accuracy\"])\n",
        "          \n",
        "          torch.save({\n",
        "                'epoch': epoch,\n",
        "                'idx': idx,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'loss': loss,\n",
        "                }, f\"/content/drive/MyDrive/rails/mit-{model_type}/last_checkpoint.pt\")\n",
        "\n",
        "    torch.save({\n",
        "            'epoch': epoch,\n",
        "            'idx': 0,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': loss,\n",
        "            }, f\"/content/drive/MyDrive/rails/mit-{model_type}/{epoch}.pt\")\n",
        "    start_idx = 0\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqG3SNZp9SxO"
      },
      "source": [
        "## Inference\n",
        "\n",
        "Finally, let's check whether the model has really learned something. Let's test the trained model on an image:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nUh8rl_rWnwW"
      },
      "outputs": [],
      "source": [
        "image = Image.open('/content/dataset/train_data/masks/'+test_dataset.images[-235])\n",
        "image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "vmBTll7G-Pr7",
        "outputId": "17ba1782-e8be-4caf-eb7e-c91f5de6576d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-e58224fc8b14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# prepare the image for the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_extractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mpixel_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'feature_extractor' is not defined"
          ]
        }
      ],
      "source": [
        "encoding = feature_extractor(image, return_tensors=\"pt\")\n",
        "pixel_values = encoding.pixel_values.to(device)\n",
        "print(pixel_values.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33ryo_eQ-dE8"
      },
      "outputs": [],
      "source": [
        "# forward pass\n",
        "outputs = model(pixel_values=pixel_values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HW_OjYNh-irt"
      },
      "outputs": [],
      "source": [
        "# logits are of shape (batch_size, num_labels, height/4, width/4)\n",
        "logits = outputs.logits.cpu()\n",
        "print(logits.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UnotB_Rd-tAF"
      },
      "outputs": [],
      "source": [
        "def ade_palette():\n",
        "    \n",
        "    return [[120, 120, 120], [180, 120, 120], [6, 230, 230], [80, 50, 50],\n",
        "            [4, 200, 3], [255, 255, 0], [140, 140, 140], [204, 5, 255],\n",
        "            [230, 230, 230], [4, 250, 7], [224, 5, 255], [235, 255, 7],\n",
        "            [150, 5, 61], [120, 120, 70], [8, 255, 51], [255, 6, 82]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oH9c-TbA-xBV"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# First, rescale logits to original image size\n",
        "upsampled_logits = nn.functional.interpolate(logits,\n",
        "                size=image.size[::-1], # (height, width)\n",
        "                mode='bilinear',\n",
        "                align_corners=False)\n",
        "\n",
        "# Second, apply argmax on the class dimension\n",
        "seg = upsampled_logits.argmax(dim=1)[0]\n",
        "color_seg = np.zeros((seg.shape[0], seg.shape[1], 3), dtype=np.uint8) # height, width, 3\n",
        "palette = np.array(ade_palette())\n",
        "for label, color in enumerate(palette):\n",
        "    color_seg[seg == label, :] = color\n",
        "# Convert to BGR\n",
        "color_seg = color_seg[..., ::-1]\n",
        "\n",
        "# Show image + mask\n",
        "img = np.array(image) * 0.5 + color_seg * 0.5\n",
        "img = img.astype(np.uint8)\n",
        "\n",
        "plt.figure(figsize=(15, 10))\n",
        "plt.imshow(img)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CoPeZTsx_MoN"
      },
      "source": [
        "Compare this to the ground truth segmentation map:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7hsBaYTY-yuH"
      },
      "outputs": [],
      "source": [
        "map = Image.open('/content/dataset/train_data/images/'+test_dataset.images[-235])\n",
        "map "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MbNeV9xdw7rm"
      },
      "outputs": [],
      "source": [
        "# convert map to NumPy array\n",
        "map = np.array(map)\n",
        "map[map == 0] = 255 # background class is replaced by ignore_index\n",
        "map = map - 1 # other classes are reduced by one\n",
        "map[map == 254] = 255\n",
        "\n",
        "classes_map = np.unique(map).tolist()\n",
        "unique_classes = [model.config.id2label[idx] if idx!=255 else None for idx in classes_map]\n",
        "print(\"Classes in this image:\", unique_classes)\n",
        "\n",
        "# create coloured map\n",
        "color_seg = np.zeros((map.shape[0], map.shape[1], 3), dtype=np.uint8) # height, width, 3\n",
        "palette = np.array(ade_palette())\n",
        "for label, color in enumerate(palette):\n",
        "    color_seg[map == label, :] = color\n",
        "# Convert to BGR\n",
        "color_seg = color_seg[..., ::-1]\n",
        "\n",
        "# Show image + mask\n",
        "img = np.array(image) * 0.5 + color_seg * 0.5\n",
        "img = img.astype(np.uint8)\n",
        "\n",
        "plt.figure(figsize=(15, 10))\n",
        "plt.imshow(img)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T7_dHF3h8kTA"
      },
      "outputs": [],
      "source": [
        "seg.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JjK3aXHp88Lf"
      },
      "outputs": [],
      "source": [
        "model.config.id2label[7]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-7pDF408siO"
      },
      "outputs": [],
      "source": [
        "np.unique(map)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oa-nRRXe85K0"
      },
      "outputs": [],
      "source": [
        "seg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zIPYabmwOgDN"
      },
      "outputs": [],
      "source": [
        "map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYwTR246pgwP"
      },
      "source": [
        "Let's compute the metrics:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQksqdF5z7q0"
      },
      "outputs": [],
      "source": [
        "# metric expects a list of numpy arrays for both predictions and references\n",
        "metrics = metric.compute(predictions=[seg.numpy()], references=[map], num_labels=16, ignore_index=255)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZTl6XuEuK_f"
      },
      "outputs": [],
      "source": [
        "metrics.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DezHZJUSqIL_"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# print overall metrics\n",
        "for key in list(metrics.keys())[:3]:\n",
        "  print(key, metrics[key])\n",
        "\n",
        "# pretty-print per category metrics as Pandas DataFrame\n",
        "metric_table = dict()\n",
        "for id, label in id2label.items():\n",
        "    metric_table[label] = [\n",
        "                           metrics[\"per_category_iou\"][id],\n",
        "                           metrics[\"per_category_accuracy\"][id]\n",
        "    ]\n",
        "\n",
        "print(\"---------------------\")\n",
        "print(\"per-category metrics:\")\n",
        "pd.DataFrame.from_dict(metric_table, orient=\"index\", columns=[\"IoU\", \"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dDdnqK_nvQUQ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import cv2\n",
        "import datetime\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "import gc\n",
        "import torch\n",
        "\n",
        "\n",
        "def video(video_path):\n",
        "\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    output_video_path = \"/content/drive/MyDrive/result2.mp4\"\n",
        "\n",
        "    capture = cv2.VideoCapture(video_path)\n",
        "    if not capture.isOpened():\n",
        "        raise Exception(\"failed to open {}\".format(video_path))\n",
        "\n",
        "    width = int(capture.get(3))\n",
        "    height = int(capture.get(4))\n",
        "\n",
        "    fourcc = cv2.VideoWriter_fourcc(*\"MP4V\")\n",
        "    fps = 30.0\n",
        "    out_video = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
        "\n",
        "    _total_ms = 0\n",
        "    count_frame = 0\n",
        "    while capture.isOpened():\n",
        "        ret, frame = capture.read()\n",
        "        count_frame += 1\n",
        "\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        start = datetime.datetime.now()\n",
        "        encoding = feature_extractor(frame, return_tensors=\"pt\")\n",
        "        pixel_values = encoding.pixel_values.to(device)\n",
        "        outputs = model(pixel_values=pixel_values)\n",
        "        logits = outputs.logits.cpu()\n",
        "        upsampled_logits = nn.functional.interpolate(logits,\n",
        "                      size=(height, width), # (height, width)\n",
        "                      mode='bilinear',\n",
        "                      align_corners=False)\n",
        "\n",
        "\n",
        "        seg = upsampled_logits.argmax(dim=1)[0]\n",
        "        color_seg = np.zeros((seg.shape[0], seg.shape[1], 3), dtype=np.uint8) # height, width, 3\n",
        "        palette = np.array(ade_palette())\n",
        "        for label, color in enumerate(palette):\n",
        "            color_seg[seg == label, :] = color\n",
        "            color_seg = color_seg[..., ::-1]\n",
        "\n",
        "        overlay = np.array(frame) * 0.5 + color_seg * 0.5\n",
        "        overlay = overlay.astype(np.uint8)\n",
        "\n",
        "        _total_ms += (datetime.datetime.now() - start).total_seconds() * 1000\n",
        "\n",
        "        out_video.write(overlay)\n",
        "\n",
        "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
        "            break\n",
        "\n",
        "        print(\"processing time one frame {}[ms]\".format(_total_ms / count_frame))\n",
        "        print(\"processing total {} frames\".format(count_frame))\n",
        "        print(\"processing total {} seconds\".format(count_frame / 30))\n",
        "        \n",
        "\n",
        "    capture.release()\n",
        "    out_video.release()\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "video(\"/content/drive/MyDrive/sakura.mp4\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2970c7949a1947579074284d81928576": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_029a14c9b4ca402fac92380939937790",
              "IPY_MODEL_92a8cb50c657418f97269ee8002d19fc",
              "IPY_MODEL_cd7be3d3e6294383811fe0a99b3b2607"
            ],
            "layout": "IPY_MODEL_5e632cafa98a4956aa2076b45e208b94"
          }
        },
        "029a14c9b4ca402fac92380939937790": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_71a65d53ef994b369f5a93f37249156d",
            "placeholder": "​",
            "style": "IPY_MODEL_a4ae7e43d9024df4bb89c77926fbccf1",
            "value": "Downloading: 100%"
          }
        },
        "92a8cb50c657418f97269ee8002d19fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3a273047763049af9ed9a41fba608dda",
            "max": 272,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6702fc57f1734c2e84863a717ae20703",
            "value": 272
          }
        },
        "cd7be3d3e6294383811fe0a99b3b2607": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c1102ac4653b4f8d9f3092d18b30d932",
            "placeholder": "​",
            "style": "IPY_MODEL_707c1970f1e84475bcb7778a71611ce6",
            "value": " 272/272 [00:00&lt;00:00, 16.5kB/s]"
          }
        },
        "5e632cafa98a4956aa2076b45e208b94": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "71a65d53ef994b369f5a93f37249156d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a4ae7e43d9024df4bb89c77926fbccf1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3a273047763049af9ed9a41fba608dda": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6702fc57f1734c2e84863a717ae20703": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c1102ac4653b4f8d9f3092d18b30d932": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "707c1970f1e84475bcb7778a71611ce6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}